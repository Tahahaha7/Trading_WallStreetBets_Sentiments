{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QT Final Project [Reddit].ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "ClwsRkPGQeCR",
        "sKL6vwlSIqLh",
        "WiivO66JFKn9",
        "DM06VmyuExAf",
        "NcCfZZqmIoNo",
        "JPJW4QEYXcnG",
        "bFCgZ1I4XteJ",
        "IWFxWe3hAk4c",
        "f6NAKIaS8pop",
        "pRMIlL_5AcpA",
        "g16xemrBojBB",
        "abQaveQhO6Wo",
        "PE6LltnJopuA"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNOaKdo6xFC5lZNDap2Lp8M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahahaha7/Trading_WallStreetBets_Sentiments/blob/main/Quantitative_Trading_with_WSB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClwsRkPGQeCR"
      },
      "source": [
        "## $\\text{Dependencies}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqOjxzYTHOzn"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oreWNJBSHlLr"
      },
      "source": [
        "# changing the working directory\n",
        "%cd /content/gdrive/My Drive/QT_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB9QraWYY1i6"
      },
      "source": [
        "%%capture\n",
        "!pip install psaw\n",
        "!pip install bayesian-optimization\n",
        "!pip install yfinance\n",
        "!pip install --upgrade nltk\n",
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no-Oh2523MWw"
      },
      "source": [
        "# API, Data, Utilities packages\n",
        "import psaw; from psaw import PushshiftAPI\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import networkx as nx\n",
        "import yfinance as yf\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL9l8OtN3szM"
      },
      "source": [
        "# Sentiment Analysis Packages\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MjK6hiyLQIq"
      },
      "source": [
        "# Plotting Packages\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "rc('text', usetex=True)\n",
        "matplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb5yWzvSLTY9"
      },
      "source": [
        "%%capture\n",
        "! sudo apt-get install texlive-latex-recommended #1\n",
        "! sudo apt-get install dvipng texlive-fonts-recommended #2\n",
        "! wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip #3\n",
        "! unzip type1cm.zip -d /tmp/type1cm #4\n",
        "! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins  #5\n",
        "! sudo mkdir /usr/share/texmf/tex/latex/type1cm #6\n",
        "! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm #7\n",
        "! sudo texhash #8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HYhZXeakrXu"
      },
      "source": [
        "%%capture\n",
        "!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng --fix-missing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKL6vwlSIqLh"
      },
      "source": [
        "## $\\text{Scraping Reddit Group}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaSoMTuF_FyD"
      },
      "source": [
        "api = PushshiftAPI()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiivO66JFKn9"
      },
      "source": [
        "### $\\text{r/wallstreetbest submissions}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VwWKmnx3sY9"
      },
      "source": [
        "'''\n",
        "THE LIST OF ATTRIBUTES RETURNED BY THE POSTS API CALL\n",
        "\n",
        "Index(['all_awardings', 'allow_live_comments', 'author',\n",
        "       'author_flair_css_class', 'author_flair_richtext', 'author_flair_text',\n",
        "       'author_flair_type', 'author_fullname', 'author_patreon_flair',\n",
        "       'author_premium', 'awarders', 'can_mod_post', 'contest_mode',\n",
        "       'created_utc', 'domain', 'full_link', 'gildings', 'id',\n",
        "       'is_crosspostable', 'is_meta', 'is_original_content',\n",
        "       'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video',\n",
        "       'link_flair_background_color', 'link_flair_css_class',\n",
        "       'link_flair_richtext', 'link_flair_template_id', 'link_flair_text',\n",
        "       'link_flair_text_color', 'link_flair_type', 'locked', 'media_metadata',\n",
        "       'media_only', 'no_follow', 'num_comments', 'num_crossposts', 'over_18',\n",
        "       'parent_whitelist_status', 'permalink', 'pinned', 'pwls',\n",
        "       'retrieved_on', 'score', 'selftext', 'send_replies', 'spoiler',\n",
        "       'stickied', 'subreddit', 'subreddit_id', 'subreddit_subscribers',\n",
        "       'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height',\n",
        "       'thumbnail_width', 'title', 'total_awards_received', 'treatment_tags',\n",
        "       'upvote_ratio', 'url', 'whitelist_status', 'wls', 'created', 'd_'],\n",
        "      dtype='object')\n",
        "''';\n",
        "\n",
        "start_time = int(datetime.datetime(2021, 1, 1).timestamp())\n",
        "\n",
        "submissions = api.search_submissions(after=start_time,\n",
        "                                     subreddit='wallstreetbets', \n",
        "                                     filter=['url', 'author', 'title', 'upvote_ratio', 'num_comments', 'score', 'subreddit'])\n",
        "\n",
        "# COLUMNS TO KEEP: ['url', 'author', 'title', 'upvote_ratio', 'num_comments', 'score', 'subreddit']\n",
        "pd.DataFrame(list(submissions)).to_csv('wallstreetbets.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoYRtR-dVBaE"
      },
      "source": [
        "data = pd.read_csv('wallstreetbets.csv')\n",
        "data.insert(2, 'time', pd.to_datetime(data.created_utc, unit='s'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8YfTt9gVE_X"
      },
      "source": [
        "content = ' '.join(map(str, data.title))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFqLbRdBU_4a"
      },
      "source": [
        "tickers = re.findall(r'[$][A-Za-z][\\S]*', content)\n",
        "tickers = ''.join(tickers)\n",
        "regex = re.compile('[^a-zA-Z]')\n",
        "tickers = regex.sub(' ', tickers).upper()\n",
        "tickers = tickers.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfO0BWbCVLjQ"
      },
      "source": [
        "top_stocks = pd.DataFrame(tickers).value_counts().rename_axis('Stock').reset_index(name='Counts')\n",
        "top_stocks.head(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM06VmyuExAf"
      },
      "source": [
        "### $\\text{r/wallstreetbets comments}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxaIBfilE2WU"
      },
      "source": [
        "'''\n",
        "Based on the mentions of stock tickers in the submissions above\n",
        "This function looks up all the comments mentioning the top 10 stocks\n",
        "Each stock has its own dataset of compiled comments\n",
        "The output is stored in the directory as a csv file\n",
        "'''\n",
        "\n",
        "for stock in top_stocks.Stock[1:11].values:\n",
        "    print('Starting {}'.format(stock))\n",
        "    gen = api.search_comments(q=stock, subreddit='wallstreetbets')\n",
        "    thing = next(gen)\n",
        "    comments = pd.DataFrame([thing.d_ for thing in gen])\n",
        "    comments.to_csv(stock.lower()+'_comments.csv', index=False)\n",
        "    print('Finishing {}'.format(stock))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t38tbIcVFDIR"
      },
      "source": [
        "'''\n",
        "THE LIST OF ATTRIBUTES RETURNED BY THE COMMENTS API CALL\n",
        "\n",
        "Index(['all_awardings', 'associated_award', 'author',\n",
        "       'author_flair_background_color', 'author_flair_css_class',\n",
        "       'author_flair_richtext', 'author_flair_template_id',\n",
        "       'author_flair_text', 'author_flair_text_color', 'author_flair_type',\n",
        "       'author_fullname', 'author_patreon_flair', 'author_premium', 'awarders',\n",
        "       'body', 'collapsed_because_crowd_control', 'comment_type',\n",
        "       'created_utc', 'gildings', 'id', 'is_submitter', 'link_id', 'locked',\n",
        "       'no_follow', 'parent_id', 'permalink', 'retrieved_on', 'score',\n",
        "       'send_replies', 'stickied', 'subreddit', 'subreddit_id',\n",
        "       'top_awarded_type', 'total_awards_received', 'treatment_tags',\n",
        "       'created', 'author_cakeday', 'media_metadata', 'distinguished',\n",
        "       'edited', 'steward_reports', 'updated_utc', 'author_created_utc',\n",
        "       'can_gild', 'collapsed', 'collapsed_reason', 'controversiality',\n",
        "       'gilded', 'nest_level', 'reply_delay', 'subreddit_name_prefixed',\n",
        "       'subreddit_type', 'score_hidden', 'rte_mode'],\n",
        "       dtype='object')\n",
        "''';\n",
        "\n",
        "# COLUMNS TO KEEP: ['created_utc', 'author', 'is_submitter', 'body', 'score']\n",
        "gme_comments = pd.read_csv('gme_comments.csv')\n",
        "gme_comments = gme_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "gme_comments.insert(1, 'time', pd.to_datetime(gme_comments.created_utc, unit='s'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xRLQfr9zqc7"
      },
      "source": [
        "gme_comments['stock'] = ['GME']*len(gme_comments)\n",
        "gme_comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4bzFVL7vWUb"
      },
      "source": [
        "\n",
        "amc_comments = pd.read_csv('amc_comments.csv')\n",
        "amc_comments = amc_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "amc_comments.insert(1, 'time', pd.to_datetime(amc_comments.created_utc, unit='s'))\n",
        "amc_comments['stock'] = ['AMC']*len(amc_comments)\n",
        "\n",
        "bb_comments = pd.read_csv('bb_comments.csv')\n",
        "bb_comments = bb_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "bb_comments.insert(1, 'time', pd.to_datetime(bb_comments.created_utc, unit='s'))\n",
        "bb_comments['stock'] = ['BB']*len(bb_comments)\n",
        "\n",
        "nok_comments = pd.read_csv('nok_comments.csv')\n",
        "nok_comments = nok_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "nok_comments.insert(1, 'time', pd.to_datetime(nok_comments.created_utc, unit='s'))\n",
        "nok_comments['stock'] = ['NOK']*len(nok_comments)\n",
        "\n",
        "sndl_comments = pd.read_csv('sndl_comments.csv')\n",
        "sndl_comments = sndl_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "sndl_comments.insert(1, 'time', pd.to_datetime(sndl_comments.created_utc, unit='s'))\n",
        "sndl_comments['stock'] = ['SNDL']*len(sndl_comments)\n",
        "\n",
        "nakd_comments = pd.read_csv('nakd_comments.csv')\n",
        "nakd_comments = nakd_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "nakd_comments.insert(1, 'time', pd.to_datetime(nakd_comments.created_utc, unit='s'))\n",
        "nakd_comments['stock'] = ['NAKD']*len(nakd_comments)\n",
        "\n",
        "slv_comments = pd.read_csv('slv_comments.csv')\n",
        "slv_comments = slv_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "slv_comments.insert(1, 'time', pd.to_datetime(slv_comments.created_utc, unit='s'))\n",
        "slv_comments['stock'] = ['SLV']*len(slv_comments)\n",
        "\n",
        "pltr_comments = pd.read_csv('pltr_comments.csv')\n",
        "pltr_comments = pltr_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "pltr_comments.insert(1, 'time', pd.to_datetime(pltr_comments.created_utc, unit='s'))\n",
        "pltr_comments['stock'] = ['PLTR']*len(pltr_comments)\n",
        "\n",
        "doge_comments = pd.read_csv('doge_comments.csv')\n",
        "doge_comments = doge_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "doge_comments.insert(1, 'time', pd.to_datetime(doge_comments.created_utc, unit='s'))\n",
        "doge_comments['stock'] = ['DOGE']*len(doge_comments)\n",
        "\n",
        "rkt_comments = pd.read_csv('rkt_comments.csv')\n",
        "rkt_comments = rkt_comments[['created_utc', 'author', 'is_submitter', 'body', 'score']]\n",
        "rkt_comments.insert(1, 'time', pd.to_datetime(rkt_comments.created_utc, unit='s'))\n",
        "rkt_comments['stock'] = ['RKT']*len(rkt_comments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxUJY_yWxSsk"
      },
      "source": [
        "all_comments = pd.concat([gme_comments, amc_comments, bb_comments, nok_comments, sndl_comments, \n",
        "                          nakd_comments, slv_comments, pltr_comments, doge_comments, rkt_comments])\n",
        "\n",
        "all_comments = all_comments.sort_values(by='created_utc').reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSiiZzWkyXIi"
      },
      "source": [
        "all_comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzEYWGumFWxo"
      },
      "source": [
        "all_comments.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dKCV1fpIzWK"
      },
      "source": [
        "import matplotlib as mpl\n",
        "x = list(map(int, all_comments.is_submitter.dropna().values[:1795600]))\n",
        "x = np.array(x).reshape(1340, 1340)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(x, interpolation='none', cmap='cividis'); plt.axis('off');\n",
        "# The ticker for being a submitter seems fairly spaced."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcCfZZqmIoNo"
      },
      "source": [
        "## $\\text{Exploratory Data Analysis}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPJW4QEYXcnG"
      },
      "source": [
        "### $\\text{r/wallstreetbest submissions}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhQBj0AaYBlj"
      },
      "source": [
        "# Create a word cloud for the group\n",
        "# Identify the authors with most influence\n",
        "# Classify the tickers for each hour\n",
        "\n",
        "pd.DataFrame(data.author.value_counts()).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIa3t2UQaryQ"
      },
      "source": [
        "keep = ['num_comments', 'score', 'upvote_ratio']\n",
        "data.groupby('author').sum().sort_values(by='num_comments', ascending=False)[keep].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aITD9WodZysd"
      },
      "source": [
        "content = ' '.join(map(str, data.title))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YtZ1quBbp0I"
      },
      "source": [
        "# WordCloud of all publications\n",
        "\n",
        "wordcloud = WordCloud(font_path='Arsenal-Regular.ttf',\n",
        "                      width = 600, height = 400,\n",
        "                      background_color ='white',\n",
        "                      stopwords = set(STOPWORDS),\n",
        "                      collocations=False,\n",
        "                      min_font_size = 10).generate(content) \n",
        "                      \n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud); plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJL3HcSid716"
      },
      "source": [
        "wordcloud = WordCloud(font_path='Arsenal-Regular.ttf', \n",
        "                      width = 600, height = 400, max_words=80,\n",
        "                      background_color ='white',\n",
        "                      collocations=False, contour_width=50,\n",
        "                      min_font_size = 10).generate(' '.join(tickers))\n",
        "                      \n",
        "plt.figure(figsize = (8, 8), facecolor = None) \n",
        "plt.imshow(wordcloud); plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFCgZ1I4XteJ"
      },
      "source": [
        "### $\\text{r/wallstreetbest comments}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40gdhAV9XvxJ"
      },
      "source": [
        "'''\n",
        "THE LOGIC: If the comment's author is a submitter, then the next comments are considered to be related to that submission\n",
        "as long as the they're not submitters. Once we encounter an author who's a submitter, then we switch to the new submission\n",
        "(following the arrow of time)\n",
        "''';\n",
        "edges = []\n",
        "current_author = np.nan\n",
        "for idx, i in enumerate(all_comments.author):\n",
        "    if all_comments.is_submitter[idx]: \n",
        "        current_author = all_comments.author[idx]\n",
        "    edges.append((i , current_author))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbh8ULaMsWNv"
      },
      "source": [
        "graph = nx.from_edgelist(edges, create_using=nx.DiGraph)\n",
        "print(nx.info(graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAQ2a-vovSLc"
      },
      "source": [
        "PageRank = pd.DataFrame(nx.pagerank(graph).items(), columns=['author', 'rank'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7pgG5nGzkHO"
      },
      "source": [
        "PageRank['clustering'] = nx.clustering(graph).values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGv1AC34wOUr"
      },
      "source": [
        "PageRank.sort_values(by='rank', ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWFxWe3hAk4c"
      },
      "source": [
        "## $\\text{Sentiment Analysis}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "temAC9HYewmm"
      },
      "source": [
        "import stanza\n",
        "stanza.download('en') # download English model\n",
        "\n",
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
        "#nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
        "\n",
        "doc = nlp(\"buy AAPL stock\")\n",
        "print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb4KI8FsX2cN"
      },
      "source": [
        "'''\n",
        "Adding new words widely used in r/wallstreetbets to improve vader's sentiment analysis\n",
        "Source: Julien Klepatch @jklepatch (https://github.com/jklepatch)\n",
        "''';\n",
        "\n",
        "wsb_words = {\n",
        "    'citron': -4.0,\n",
        "    'hidenburg': -4.0,\n",
        "    'moon': 4.0,\n",
        "    'highs': 2.0,\n",
        "    'mooning': 4.0,\n",
        "    'long': 2.0,\n",
        "    'short': -2.0,\n",
        "    'call': 4.0,\n",
        "    'calls': 4.0,\n",
        "    'put': -4.0,\n",
        "    'puts': -4.0,\n",
        "    'break': 2.0,\n",
        "    'tendie': 2.0,\n",
        "    'tendies': 2.0,\n",
        "    'town': 2.0,\n",
        "    'overvalued': -3.0,\n",
        "    'undervalued': 3.0,\n",
        "    'buy': 4.0,\n",
        "    'sell': -4.0,\n",
        "    'gone': -1.0,\n",
        "    'gtfo': -1.7,\n",
        "    'paper': -1.7,\n",
        "    'bullish': 3.7,\n",
        "    'bearish': -3.7,\n",
        "    'bagholder': -1.7,\n",
        "    'stonk': 1.9,\n",
        "    'green': 1.9,\n",
        "    'money': 1.2,\n",
        "    'print': 2.2,\n",
        "    'rocket': 2.2,\n",
        "    'bull': 2.9,\n",
        "    'bear': -2.9,\n",
        "    'pumping': -1.0,\n",
        "    'sus': -3.0,\n",
        "    'offering': -2.3,\n",
        "    'rip': -4.0,\n",
        "    'downgrade': -3.0,\n",
        "    'upgrade': 3.0,     \n",
        "    'maintain': 1.0,          \n",
        "    'pump': 1.9,\n",
        "    'hot': 1.5,\n",
        "    'drop': -2.5,\n",
        "    'rebound': 1.5,  \n",
        "    'crack': 2.5,}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C5fP_zJlozs"
      },
      "source": [
        "# Create a sentiment for GME for every hour of 2021 (make it generalizable to other stocks)\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "vader.lexicon.update(wsb_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQOpW0dQ4Emf"
      },
      "source": [
        "# Keep only the comment from 2021\n",
        "all_comments_2021 = all_comments[all_comments.time.dt.year == 2021]\n",
        "all_comments_2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZCfLaH7185Q"
      },
      "source": [
        "for stock in top_stocks.Stock[1:10]:\n",
        "    print('Starting {}'.format(stock))\n",
        "    # Pick the stock of interest\n",
        "    store = all_comments_2021[all_comments_2021.stock == stock]\n",
        "\n",
        "    # Combining all the comments on WSB for every half an hour since Jan 1st 2021 to Apr 4th 2021\n",
        "    agg_comments_2021 = store[['time', 'body']].groupby(pd.Grouper(key='time', freq='30min'))['body'].apply(lambda x: ' '.join(x)).reset_index()\n",
        "    \n",
        "    # Sentiment analysis of the comments\n",
        "    sentiment = agg_comments_2021['body'].apply(vader.polarity_scores)\n",
        "    processed_data = pd.DataFrame(list(sentiment))\n",
        "    processed_data['time'] = agg_comments_2021['time']\n",
        "    processed_data['stock'] = [stock]*len(processed_data)\n",
        "    processed_data.to_csv(stock.lower()+'_sentiment.csv', index=False)\n",
        "    print('Finished {}'.format(stock))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJGoKkm_7tp-"
      },
      "source": [
        "gme_sentiment = pd.read_csv('gme_sentiment.csv')\n",
        "gme_sentiment['stock'] = ['GME']*len(gme_sentiment)\n",
        "gme_sentiment['time'] = all_comments_2021[all_comments_2021.stock == 'GME'][['time', 'body']]\\\n",
        "                        .groupby(pd.Grouper(key='time', freq='30min'))['body'].apply(lambda x: ' '.join(x)).reset_index()['time']\n",
        "\n",
        "amc_sentiment = pd.read_csv('amc_sentiment.csv')\n",
        "#amc_sentiment['stock'] = ['AMC']*len(amc_sentiment)\n",
        "\n",
        "bb_sentiment = pd.read_csv('bb_sentiment.csv')\n",
        "#bb_sentiment['stock'] = ['BB']*len(bb_sentiment)\n",
        "\n",
        "nok_sentiment = pd.read_csv('nok_sentiment.csv')\n",
        "#nok_sentiment['stock'] = ['NOK']*len(nok_sentiment)\n",
        "\n",
        "sndl_sentiment = pd.read_csv('sndl_sentiment.csv')\n",
        "#sndl_sentiment['stock'] = ['SNDL']*len(sndl_sentiment)\n",
        "\n",
        "nakd_sentiment = pd.read_csv('nakd_sentiment.csv')\n",
        "#nakd_sentiment['stock'] = ['NAKD']*len(nakd_sentiment)\n",
        "\n",
        "slv_sentiment = pd.read_csv('slv_sentiment.csv')\n",
        "#slv_sentiment['stock'] = ['SLV']*len(slv_sentiment)\n",
        "\n",
        "pltr_sentiment = pd.read_csv('pltr_sentiment.csv')\n",
        "#pltr_sentiment['stock'] = ['PLTR']*len(pltr_sentiment)\n",
        "\n",
        "doge_sentiment = pd.read_csv('doge_sentiment.csv')\n",
        "#doge_sentiment['stock'] = ['DOGE']*len(doge_sentiment)\n",
        "\n",
        "rkt_sentiment = pd.read_csv('rkt_sentiment.csv')\n",
        "#rkt_sentiment['stock'] = ['RKT']*len(rkt_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRzPi15Up0XS"
      },
      "source": [
        "all_sentiment = pd.concat([gme_sentiment, amc_sentiment, bb_sentiment, nok_sentiment, \n",
        "                           sndl_sentiment, nakd_sentiment, slv_sentiment, pltr_sentiment, \n",
        "                           doge_sentiment, rkt_sentiment]).reset_index(drop=True)\n",
        "\n",
        "all_sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IFY9BLiMSAE"
      },
      "source": [
        "all_sentiment.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNj5WqCKKN4o"
      },
      "source": [
        "plt.figure(figsize=(7, 15))\n",
        "titles = ['Negative sentiment', 'Neutral sentiment', 'Positive sentiment']\n",
        "for idx,i in enumerate(all_sentiment.columns[:3]):\n",
        "    plt.subplot(3, 1, idx+1)\n",
        "    plt.title(titles[idx], fontsize=20)\n",
        "    plt.hist(all_sentiment[all_sentiment[i] != 0][i], bins='auto', alpha=.7, density=False)\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6NAKIaS8pop"
      },
      "source": [
        "## $\\text{Hourly Stock Market Data}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgrlVsc4JFQV"
      },
      "source": [
        "# Get hourly data for top 10 stocks mentioned in the group\n",
        "\n",
        "for idx, stock in enumerate(top_stocks.Stock[:10]):\n",
        "    print(idx, stock)\n",
        "    hourly_data_1 = yf.download(tickers=stock, \n",
        "                                start=datetime.datetime(2021, 1, 1),\n",
        "                                end=datetime.datetime(2021, 2, 9),\n",
        "                                interval=\"60m\")\n",
        "    hourly_data_1['stock'] = [stock] * len(hourly_data_1)\n",
        "    hourly_data_2 = yf.download(tickers=stock, \n",
        "                                start=datetime.datetime(2021, 2, 10),\n",
        "                                end=datetime.datetime(2021, 4, 2),\n",
        "                                interval=\"30m\")\n",
        "    hourly_data_2['stock'] = [stock] * len(hourly_data_2)\n",
        "\n",
        "    if idx == 0:\n",
        "        hourly_data = pd.concat([hourly_data_1, hourly_data_2])\n",
        "    else:\n",
        "        hourly_data = pd.concat([hourly_data, hourly_data_1, hourly_data_2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy9sZmYB8TnS"
      },
      "source": [
        "hourly_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG4-7CWGbMuX"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(hourly_data['Adj Close'], 'k.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1VTJfTZXsMK"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(sentiment['compound'], 'k.');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q86Gv-7gpdX3"
      },
      "source": [
        "all_sentiment.index = all_sentiment.time\n",
        "all_sentiment.index.rename('DateTime', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL4sT811Pbu7"
      },
      "source": [
        "all_sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LmQmygTqw8v"
      },
      "source": [
        "# Prob keep only the sentiment during trading hours\n",
        "\n",
        "hourly_data.index = hourly_data.index.tz_convert('UTC').tz_convert(None)\n",
        "\n",
        "hourly_data = hourly_data.reset_index()\n",
        "\n",
        "hourly_data = hourly_data.rename(columns={'index':'time'})\n",
        "\n",
        "hourly_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvRZFcYdZMeN"
      },
      "source": [
        "all_sentiment['time'] = pd.to_datetime(all_sentiment['time'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNur62iZp0th"
      },
      "source": [
        "# Test for coorelation between sentiment and stock data\n",
        "#full_data = pd.merge(sentiment, hourly_data, left_index=True, right_index=True, how='inner')\n",
        "\n",
        "full_data = pd.merge(all_sentiment, hourly_data, left_on=['time', 'stock'], right_on=['time', 'stock'], how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0UwNwlI-hw5"
      },
      "source": [
        "# The data starts with 1h interval then goes to 30min inteval for the last 2 months\n",
        "# The sentiment can be lagged by one time step to test its predictibility\n",
        "full_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJ2x7kpQ3Sa"
      },
      "source": [
        "plt.figure(figsize=(17, 6))\n",
        "\n",
        "all_sentiment_plot = all_sentiment[all_sentiment.compound != 0].reset_index(drop=True)\n",
        "\n",
        "plt.stackplot(all_sentiment_plot.index, all_sentiment_plot.pos, all_sentiment_plot.neu, all_sentiment_plot.neg, \n",
        "              labels=['{i}'.format(i=i) for i in ['Negative', 'Neutral', 'Positive']])\n",
        "plt.legend(loc='best', fontsize=16); plt.xlabel('Time Step', fontsize=20)\n",
        "plt.ylabel('Sentiment', fontsize=20); plt.xlim(0, 22800); plt.ylim(-.1, 1.1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRMIlL_5AcpA"
      },
      "source": [
        "## $\\text{Trading strategies}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g16xemrBojBB"
      },
      "source": [
        "### $\\text{Gradient Boosting: Nested Cross-Validation}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDA3QRapcGgE"
      },
      "source": [
        "full_data['stock'] = full_data.stock.astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe6q9GNocMKc"
      },
      "source": [
        "full_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crDwhIMEdwXW"
      },
      "source": [
        "encoding = dict( zip( full_data['stock'], full_data['stock'].cat.codes ) )\n",
        "\n",
        "full_data['stock_encoding'] = [encoding.get(i) for i in full_data.stock]\n",
        "full_data['stock_encoding'] = full_data['stock_encoding'].astype('category')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeLt2qRdfqTx"
      },
      "source": [
        "full_data.stock_encoding.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc4HV956YNAt"
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "data_X = full_data.drop(labels=['compound', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'time', 'stock'], axis=1).values\n",
        "data_Y = full_data['Adj Close'].pct_change(1).apply(lambda x : 1 if x > 0 else 0).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-devVK9eC0Q2"
      },
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, auc\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, plot_confusion_matrix\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier()\n",
        "tscv = TimeSeriesSplit(n_splits = 7)\n",
        "\n",
        "for train_index, test_index in tscv.split(data_X):\n",
        "    X_train, X_test = data_X[train_index], data_X[test_index]\n",
        "    y_train, y_test = data_Y[train_index], data_Y[test_index]\n",
        "\n",
        "    lgb_model.fit(X_train, y_train.reshape(len(y_train),), categorical_feature='auto', verbose=True, )\n",
        "    y_pred = lgb_model.predict(X_test)\n",
        "    print('Training set score: {:.4f}'.format(lgb_model.score(X_train, y_train)))\n",
        "    print('Test set score: {:.4f}\\n'.format(lgb_model.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fce5mcJoZGrT"
      },
      "source": [
        "def plot_cm(cm):\n",
        "    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
        "    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
        "    group_percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n",
        "              zip(group_names, group_counts, group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    sns.set(rc={'figure.figsize':(8, 6)})\n",
        "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', alpha=.75)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh1dcBdiO5_O"
      },
      "source": [
        "plot_cm(confusion_matrix(y_test, y_pred));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abQaveQhO6Wo"
      },
      "source": [
        "### $\\text{Gradient Boosting: Hypterparameter Tuning}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ArC0L1FZYa1"
      },
      "source": [
        "def lgbm_cv(max_depth, learning_rate, \n",
        "            n_estimators, reg_alpha,\n",
        "            bagging_fraction, \n",
        "            min_child_weight, min_split_gain,\n",
        "            colsample_bytree):\n",
        "    \n",
        "    estimator_function = lgb.LGBMClassifier(max_depth=int(max_depth),\n",
        "                                            learning_rate = learning_rate,\n",
        "                                            bagging_fraction = bagging_fraction,\n",
        "                                            min_child_weight = min_child_weight,\n",
        "                                            min_split_gain = min_split_gain,\n",
        "                                            colsample_bytree = colsample_bytree,\n",
        "                                            n_estimators = int(n_estimators),\n",
        "                                            reg_alpha = reg_alpha, nthread = -1,\n",
        "                                            objective = 'binary', seed = 42)\n",
        "    \n",
        "    estimator_function.fit(X_train, y_train.reshape(len(y_train),))\n",
        "    probs = estimator_function.predict_proba(X_test)[:,1]\n",
        "    return roc_auc_score(y_test, probs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzh3oHF7ZYWi"
      },
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "gp_params = {\"alpha\": 1e-10}\n",
        "\n",
        "hp_space = {\n",
        "    'max_depth': (5, 30),\n",
        "    'learning_rate': (.001, 1),\n",
        "    'n_estimators' : (10, 250),\n",
        "    'reg_alpha': (0, 1),\n",
        "    'bagging_fraction': (.8, 1),\n",
        "    'min_child_weight': (1, 25),\n",
        "    'min_split_gain': (.001, .1),\n",
        "    'colsample_bytree': (.1, 1)\n",
        "}\n",
        "\n",
        "lgbcBO = BayesianOptimization(f=lgbm_cv, pbounds=hp_space, random_state=42, verbose=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRRohCpLZYTx"
      },
      "source": [
        "lgbcBO.maximize(init_points=5, n_iter=45, acq='ucb', kappa= 3, **gp_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyALTxlIZdrz"
      },
      "source": [
        "optimal_params = lgbcBO.res[np.argmax(pd.DataFrame(lgbcBO.res)['target'])]['params']\n",
        "\n",
        "print(optimal_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JDCnoezZhoV"
      },
      "source": [
        "gbm = lgb.LGBMClassifier(**{'bagging_fraction': 0.8783378533433686, \n",
        "                            'colsample_bytree': 0.6728458365247224, \n",
        "                            'learning_rate': 0.7991630583627533, \n",
        "                            'max_depth': 29, \n",
        "                            'min_child_weight': 24.799522961470622, \n",
        "                            'min_split_gain': 0.04761293422236514, \n",
        "                            'n_estimators': 64, \n",
        "                            'reg_alpha': 0.7738117598769145})\n",
        "\n",
        "gbm.fit(X_train, y_train.reshape(len(y_train),), verbose=True)\n",
        "\n",
        "print('Training set score: {:.4f}'.format(gbm.score(X_train, y_train)))\n",
        "print('Test set score: {:.4f}\\n'.format(gbm.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6hPBIVdZhk1"
      },
      "source": [
        "plot_cm(confusion_matrix(y_test, y_pred));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE6LltnJopuA"
      },
      "source": [
        "### $\\text{LSTM}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQoX_5RQouFX"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n3QeFkBT79G"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(64, input_shape=(5, 1), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(LSTM(16))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAJ32AxFT_n0"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.002, decay=1e-5)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-lyC3mMslf3"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjjO3OiLUQqy"
      },
      "source": [
        "EPOCHS = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_x = X_train.reshape(5058, 5, 1)\n",
        "train_y = y_train\n",
        "\n",
        "history = model.fit(\n",
        "    train_x, train_y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NevT6_bNUgmW"
      },
      "source": [
        "score = model.evaluate(X_test.reshape(722, 5, 1), y_test, verbose=True)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}